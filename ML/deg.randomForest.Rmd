---
title: "Random Forest using DEGs"
output: html_document
---

Doing the same things as those in glmnet but using random forest
```{R}
suppressPackageStartupMessages({
  library(randomForest)
  library(FSelector)
  library(caret)
  library(dplyr)
  library(caTools)
  library(gplots)
  library(pROC)
  library(ggpubr)
})
```

## Cross-validation on different feature sets
### Load data & feature sets
```{R}
# Get the filtered tpm table & cdr first (6125 subjects)
txi <- readRDS("/home/ubuntu/hdd/tcga/txi.split/filted.txi.rds")
tpm <- txi[["abundance"]]
cdr <- read.csv("/home/ubuntu/hdd/tcga/clinic/new.filt.cdr.csv", header = T, row.names = 1)

# Load the DEG feature sets
fs_dir <- "/home/ubuntu/hdd/tcga/machines/features/deg"
featureSets <- list()
for(x in 1:6){ 
  set <- read.delim(file.path(fs_dir, paste0("feature.", x, ".list")), header = F)
  featureSets[[as.character(x)]] <- as.vector(set$V1)
}

# Filter the features and merge tpm table and labels
tpm <- log2(tpm + 1)
tpm <- as.data.frame(t(tpm))
tpm$sample_barcode <- rownames(tpm)
dataset <- merge(tpm, cdr[ ,c(2,3,35)], by="sample_barcode") # sample_barcode & Metastasis columns in cdr
dataset$Metastasis <- as.factor(dataset$Metastasis)

# Split the dataset into 90% training and 10% testing
set.seed(88)
sample <- sample.split(dataset$Metastasis, SplitRatio = 0.9)
base_train <- subset(dataset, sample==T)
base_test <- subset(dataset, sample==F)
base_test_labels <- base_test$Metastasis

# Ensemble different resampled datasets
all_noM <- base_train %>% filter(Metastasis==0)
train_M <- base_train %>% filter(Metastasis==1)

ensemblLists <- function(imbalanced0, imbalanced1, numOfList){
  trainlists <- list()
  t <- 1
  while(t < numOfList){
    sample <- sample.split(imbalanced0$sample_barcode, SplitRatio=nrow(imbalanced1)/nrow(imbalanced0))
    train_noM <- subset(imbalanced0, sample==T)
    imbalanced0 <- subset(imbalanced0, sample==F)
    trainlists[[as.character(t)]] <- list()
    trainlists[[as.character(t)]]$tpm <- rbind(imbalanced1, train_noM)
    trainlists[[as.character(t)]]$labels <- as.data.frame(trainlists[[as.character(t)]]$tpm$Metastasis)
    colnames(trainlists[[as.character(t)]]$labels) <- "label"
    t <- t + 1
  }

  # Add the 13th training set
  trainlists[[as.character(numOfList)]] <- list()
  trainlists[[as.character(numOfList)]]$tpm <- rbind(imbalanced1, imbalanced0)
  trainlists[[as.character(numOfList)]]$labels <- as.data.frame(trainlists[[as.character(numOfList)]]$tpm$Metastasis)
  colnames(trainlists[[as.character(numOfList)]]$labels) <- "label"
  trainlists
}
  
trainlists <- ensemblLists(all_noM, train_M, 13)
```

# Models building
```{R}
# Function to get ACC, SN, SP, F1, MCC, AUC, BAC
perf_results <- function(actual, predicted, predictionScores, model){
 results <- list()
 cm <- confusionMatrix(data=as.factor(predicted), reference=as.factor(actual), positive="1")
 results[['ACC']] <- as.numeric(cm$overall['Accuracy'])
 results[['SN']] <- as.numeric(cm$byClass['Sensitivity'])
 results[['SP']] <- as.numeric(cm$byClass['Specificity'])
 results[['F1']] <- as.numeric(cm$byClass['F1'])
 results[['MCC']] <- ((cm$table[1,1]*cm$table[2,2])-(cm$table[1,2]*cm$table[2,1]))/(sqrt(cm$table[1,1]+cm$table[1,2])*sqrt(cm$table[1,1]+cm$table[2,1])*sqrt(cm$table[2,2]+cm$table[1,2])*sqrt(cm$table[2,2]+cm$table[2,1]))
 results[['AUC']] <- auc(roc(response=as.vector(actual), predictor=as.vector(predictionScores)))
 results[['BAC']] <- as.numeric(cm$byClass['Balanced Accuracy'])
 results[['model']] <- model
 #results[['ROC']] <- plot.roc(roc(as.vector(actual), as.vector(predictionScores)))
 results
}

# A function for usage in each training set
randomForestCV <- function(trainlist, featureSets){
  results.list <- list()
  parameters.list <- list()
  
  tpm <- trainlist[["tpm"]]
  labels <- trainlist[["labels"]]
  
  for(fs in 1:length(featureSets)){
    results.list[[as.character(fs)]] <- list()
    parameters.list[[as.character(fs)]] <- list()
    
    #Subset TPM
    cv_train <- tpm[, which(names(tpm) %in% c("type", "Metastasis", featureSets[[as.character(fs)]]))]
    
    # Create 5 folds
    folds <- createFolds(labels$label, k=5)
    for(i in 1:5){
      results.list[[as.character(fs)]][[i]] <- list()
      parameters.list[[as.character(fs)]][[i]] <- list()
      
      # 1/5 for validation, 4/5 for training
      sub_cv_train <- cv_train[-folds[[i]],]
      sub_cv_train_labels <- labels[-folds[[i]],]
      sub_cv_validate <- cv_train[folds[[i]],]
      sub_cv_validate_labels <- labels[folds[[i]],]
      
      # Initialize model & parameters
      rf_model <- randomForest(Metastasis~., data=sub_cv_train, ntree=1001) 
      predictionLabels <- predict(rf_model, newdata=sub_cv_validate, type='response')
      predictionPrs <- predict(rf_model, newdata=sub_cv_validate, type='prob')
      results.list[[as.character(fs)]][[i]] <- perf_results(sub_cv_validate_labels, predictionLabels, predictionPrs[,1], rf_model)
      
      parameters.list[[as.character(fs)]][[i]]$n <- 1001
      #parameters.list[[as.character(fs)]][[i]]$AUC <- results.list[[as.character(fs)]][[i]]$AUC
      parameters.list[[as.character(fs)]][[i]]$model <- results.list[[as.character(fs)]][[i]]$model
      
      # For parameter optimization -- grid search
      # print("Optimizing parameters...")
      # for(n in seq(101, 1001, by=100)){
      #   # Build rf model with tuned mtry for specified n trees
      #   tuned_mtry <- (tuneRF(cv_train[,1:ncol(cv_train)-1], cv_train_labels, ntreeTry=n, stepFactor=1.5, improve=0.01, trace=FALSE, plot=FALSE, doBest=TRUE))$mtry
      #   rf_model_tuned <- randomForest(Metastasis~., data=cv_train, ntree=n, mtry=tuned_mtry)
      #   
      #   # Test model with testing dataset
      #   predictionLabels <- predict(rf_model_tuned, newdata=cv_validate, type='response')
      #   predictionPrs <- predict(rf_model_tuned, newdata=cv_validate, type='prob')
      #   results_op <- perf_results(cv_validate_labels, predictionLabels, predictionPrs[,1], rf_model_tuned)
      #   
      #   # Update everything if better
      #   if(results_op$AUC > parameters.list[[i]]$AUC){
      #     parameters.list[[i]]$AUC
      #     parameters.list[[i]]$model <- rf_model_tuned
      #     parameters.list[[i]]$n <- n
      #     results.list[[i]] <- results_op
      #   }
      # }
      
      # Print the best performance for each fold
      print(paste0("Fold ", i, " using ", parameters.list[[as.character(fs)]][[i]]$n, " trees"))
      print(paste0("  AUC:", round(results.list[[as.character(fs)]][[i]]$AUC, 6), ";", 
                   "  MCC: ", round(results.list[[as.character(fs)]][[i]]$MCC,6), ";",
                   "  BAC: ", round(results.list[[as.character(fs)]][[i]]$BAC,6)))
    }
  }
  list(results.list=results.list, parameters.list=parameters.list)
}

# Loop for ensembled training sets
RF.lists <- list()
for(i in 1:length(trainlists)){
  print(paste0("=========================Trainlist ", i, "========================="))
  RF.lists[[as.character(i)]] <- randomForestCV(trainlists[[as.character(i)]], featureSets)
}


```

Read feature selection [information gain](https://stackoverflow.com/questions/33425824/why-information-gain-feature-selection-gives-zero-scores).

### Test on independent testing set 
Well, just using the testing set only is enough.
```{R}
testRF <- function(testset, parameters.list, featureSets){
  # Find the best model from every 5 folds of every 6 feature sets
  # Find the best fold from all trainlists
  test.list <- list()
  test.list$bestAUC <- 0
  test.list$bestFS <- 0
  test.list$bestModel <- NA
  
  for(fs in 1:length(featureSets)){
    test.list[[as.character(fs)]] <- list()
    test.list[[as.character(fs)]]$bestAUC <- 0
    
    # Subset expression data
    sub_test <- testset[, which(names(testset) %in% c("type", "Metastasis", featureSets[[as.character(fs)]]))]
    sub_test_labels <- testset$Metastasis
    
    # loop the folds
    for(i in 1:5){
      model <- parameters.list[[as.character(fs)]][[i]]$model
      # Test on testing set
      predictionLabels_test <- predict(model, newdata=sub_test, type='response')
      predictionPrs_test <- predict(model, newdata=sub_test, type='prob')
      test_results <- perf_results(sub_test_labels, predictionLabels_test, predictionPrs_test[,1], model)
      # Find the highest AUC from models trained in 5 folds
      if(test_results$AUC > test.list[[as.character(fs)]]$bestAUC){
        test.list[[as.character(fs)]]$bestAUC <- test_results$AUC
        test.list[[as.character(fs)]]$bestBAC <- test_results$BAC
        test.list[[as.character(fs)]]$bestMCC <- test_results$MCC
        test.list[[as.character(fs)]]$results <- test_results
        test.list[[as.character(fs)]]$bestModel <- model
      }
    }
    # Find the highest AUC from models trained in 6 featuresets
    if(test.list[[as.character(fs)]]$bestAUC > test.list$bestAUC){
      test.list$bestAUC <- test.list[[as.character(fs)]]$bestAUC
      test.list$bestFS <- fs
      test.list$bestModel <- test.list[[as.character(fs)]][['bestModel']]
    }
  }
  print(paste0("  The best FS: ", test.list$bestFS, ", AUC ", round(test.list$bestAUC, 5)))
  test.list
}

test.results <- list()
test.results$bestAUC <- 0

for(i in 1:length(trainlists)){
  print(paste0("Trainlist", i))
  test.results[[as.character(i)]] <- testRF(base_test, RF.lists[[as.character(i)]][["parameters.list"]], featureSets)
  if(test.results[[as.character(i)]]$bestAUC > test.results$bestAUC){
    test.results$bestAUC <- test.results[[as.character(i)]]$bestAUC
    test.results$bestModel <- test.results[[as.character(i)]]$bestModel
    test.results$bestFS <- test.results[[as.character(i)]]$bestFS
  }
}

print(paste("Best Testing AUC: ", test.results$bestAUC, " with best set", test.results$bestFS))

```

### Draw the performances
```{R}
draw_CCC <- function(results.list){
  
  # AUC
  auc_df <- data.frame(fs1 = rep(0,length(trainlists)), fs2 = rep(0,length(trainlists)),
                         fs3 = rep(0,length(trainlists)), fs4 = rep(0,length(trainlists)),
                         fs5 = rep(0,length(trainlists)), fs6 = rep(0,length(trainlists)))

  for(list in 1:length(trainlists)){
    for(fs in 1:length(featureSets)){
      auc_df[list,][fs] <- results.list[[as.character(list)]][[as.character(fs)]][["bestAUC"]]
    }
  }
  auc_p <- ggplot(stack(auc_df), aes(x=ind, y=values, fill=ind)) +
    geom_boxplot() + scale_y_continuous(limits=c(0,1)) +
    labs(title="AUC", x="", y="", fill="Sets") + scale_fill_brewer(palette="Dark2") +
    theme_classic()
  
  # BAC
  bac_df <- data.frame(fs1 = rep(0,length(trainlists)), fs2 = rep(0,length(trainlists)),
                         fs3 = rep(0,length(trainlists)), fs4 = rep(0,length(trainlists)),
                         fs5 = rep(0,length(trainlists)), fs6 = rep(0,length(trainlists)))

  for(list in 1:length(trainlists)){
    for(fs in 1:length(featureSets)){
      bac_df[list,][fs] <- results.list[[as.character(list)]][[as.character(fs)]][["bestBAC"]]
    }
  }
  bac_p <- ggplot(stack(bac_df), aes(x=ind, y=values, fill=ind)) +
    geom_boxplot() + scale_y_continuous(limits=c(0,1)) +
    labs(title="BAC", x="", y="", fill="Sets") + scale_fill_brewer(palette="Dark2") +
    theme_classic()
  
  # MCC
  mcc_df <- data.frame(fs1 = rep(0,length(trainlists)), fs2 = rep(0,length(trainlists)),
                         fs3 = rep(0,length(trainlists)), fs4 = rep(0,length(trainlists)),
                         fs5 = rep(0,length(trainlists)), fs6 = rep(0,length(trainlists)))

  for(list in 1:length(trainlists)){
    for(fs in 1:length(featureSets)){
      mcc_df[list,][fs] <- results.list[[as.character(list)]][[as.character(fs)]][["bestMCC"]]
    }
  }
  mcc_p <- ggplot(stack(mcc_df), aes(x=ind, y=values, fill=ind)) +
    geom_boxplot() + scale_y_continuous(limits=c(0,1)) +
    labs(title="MCC", x="", y="", fill="Sets") + scale_fill_brewer(palette="Dark2") +
    theme_classic()

  ggarrange(auc_p, bac_p, mcc_p, ncol=3, nrow=1, common.legend=T, legend = "right")
}

draw_CCC(test.results)
```


### Results analyses

```{R}
final.model <- test.results$bestModel
final.FS <- test.results$bestFS
final.test <- base_test[, which(names(base_test) %in% c("type", "Metastasis",  featureSets[[as.character(final.FS)]]))]
final.predictionLabels <- predict(final.model, newdata=final.test, type='response')
final.predictionScores <- predict(final.model, newdata=final.test, type='prob')
```

#### ROC curve
```{R roc}
plot.roc(roc(as.numeric(base_test_labels), as.vector(final.predictionScores[,1])))
auc(roc(as.vector(base_test_labels), as.numeric(final.predictionScores[,1])))
confusionMatrix(as.factor(final.predictionLabels), as.factor(base_test_labels), positive="1")
```

### Test the model in each tissue
```{R}
all.type <- as.character(unique(base_test$type))
all.type <- all.type[! all.type %in% c("CHOL", "GBM", "KICH", "KIRC", "KIRP", "LGG", "LIHC", "OV")]
testset <- base_test[,which(names(base_test) %in% c("type", "Metastasis", featureSets[[as.character(final.FS)]]))]

#test.results <- rf.deg.test

final.method <- "RandomForest"
single_test <- function(tissue, testset){
  single.results <- list()
  for(t in tissue){
    s.test <- testset %>% filter(type == t)
    s.label <- s.test$Metastasis
    if(1 %in% s.label){
      single.results[[t]] <- list()
      s.predictionLables <- predict(final.model, newdata=s.test, type='response')
      s.predictionPrs <- predict(final.model, newdata=s.test, type='prob')
      s.test.result <- perf_results(s.label, s.predictionLables, s.predictionPrs[,1], final.method)
      print(t)
      print(paste0("   AUC ", round(s.test.result$AUC, 4), " ;", 
                paste0("BAC ", round(s.test.result$BAC, 4)), " ;",
                paste0("MCC ", round(s.test.result$MCC, 4), " ;")))
      single.results[[t]] <- s.test.result
    }
  }
  single.results
}

single.results <- single_test(all.type, testset)

```



#### Distribution of prediction scores
```{R distribution}
tmp.scores <- as.data.frame(final.predictionScores[,1])
colnames(tmp.scores) <- "predictionScores"

ggplot(tmp.scores, aes(x=predictionScores)) +
  geom_density() +
  ggtitle("Distribution of metastasis predictions scores") +
  coord_cartesian(xlim=c(0,1)) +
  theme_minimal()

```

#### Important genes
```{R impt}
# Mean decrease in node impurity plots
fData <- read.delim("/home/ubuntu/hdd/tcga/fData.txt", header = T, row.names = 1)
impdf <- varImp(test.results$bestModel)
impdf$varNames <- rownames(impdf)
impdf <- impdf[order(impdf$Overall, decreasing = T), ]

assignName <- function(v){
  if(v %in% rownames(fData)){
    name <- as.character(fData[v,]$geneNames)
  }else{
    name <- v
  }
  name
}

impdf$varNames <- sapply(impdf$varNames, assignName)
#imp_labels <- impdf$varNames[1:30]
#varImpPlot(test.results$bestModel, type=2, labels = rev(imp_labels))

ggplot(impdf[1:40,], aes(x=reorder(varNames, Overall), y=Overall)) +
  geom_point() + 
  coord_flip() +
  ggtitle("Variables with the non-zero coefficients") +
  theme_minimal()
```


### Save
```{r}
result_dir <- "/home/ubuntu/hdd/tcga/machines/results/randomForest/deg"
saveRDS(RF.lists, file.path(result_dir, "RF.lists.rds"))
saveRDS(test.results, file.path(result_dir, "test.list.rds"))
```
